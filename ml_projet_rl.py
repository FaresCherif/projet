# -*- coding: utf-8 -*-
"""ML_Projet_RL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1brtFx5xAeuICF8G4odlFA-nfdBnp3MLw
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy.io as sc
import scipy
import sklearn as sklearn
from random import randint, random
import matplotlib.pyplot as plt


dragons=[4,6,11,13]
jewels = [15]
fin = dragons+jewels
malus = -1
malus_hors_borne = 0
recompense_vide = 0
recompense = 1
longueur = 4
largeur = 4
taille = longueur * largeur
position_initial = 0
iteration = 0
NPartie = 10
alpha = 0.81
gamma = 0.96
nbEchantillons = 10

def space(T):
  
  plateau = np.ones(T)*recompense_vide
  for dragon in dragons : 
    plateau[dragon] = malus
  
  for jewel in jewels :
    plateau[jewel] = recompense
  
  return plateau

def actionResult(A,position):
  if A==0 : # Haut
    if position[0] !=0 :
      return [-1,0]
    else :
      return [0,0]
  
  elif A == 1 : # Droite
    if position[1] !=longueur-1 :
      return [0,1]
    else :
      return [0,0]

  elif A==2 : # Bas
    if position[0] !=largeur-1 :
      return [1,0]
    else :
      return [0,0]

  elif A == 3 : # Gauche
    if position[1] !=0 :
      return [0,-1]
    else :
      return [0,0]

  else :
    print("pas une bonne action")


def application_action(action,position,space) :
  pos_matrix = [ position // largeur,position%largeur ]
  movement = actionResult(action,pos_matrix)
  nouvellePosition = [movement[0]+pos_matrix[0],movement[1]+pos_matrix[1]]
  reward = space[nouvellePosition[0]][nouvellePosition[1]]
  if movement==[0,0] :
    reward = malus_hors_borne
  position_finale = nouvellePosition[0]*largeur + nouvellePosition[1]
  isfin = position_finale in fin
  return position_finale,reward,isfin


#state est la position du joueur
#epsilon cot√© random / suivre politique
#mat_Q c'est la politique
def choose_action(state,epsilon,mat_q) :
  alea = random()
  if(alea <= epsilon) :
    return randint(0, 3)
  else :
    choix_possibles = mat_q[state]
    action = np.argmax(choix_possibles)
    return  action


def random_mat_q():
  #mat_q = np.random.randint(3, size=(16, 4))
  mat_q = np.zeros((16, 4))
  return mat_q

def onestep(mat_q,state,epsilon):
  action = choose_action(state,epsilon,mat_q)

  new_state,reward,fin = application_action(action,state,plateau)
  #print("avant : ",mat_q[state][action])
  #print(reward+gamma*np.max(mat_q[new_state])-mat_q[state][action])
  mat_q[state][action]=mat_q[state][action]+alpha*(reward+gamma*np.max(mat_q[new_state])-mat_q[state][action])
  #print("apres : ",mat_q[state][action])
  #print("a : ",action," s : ",new_state)
  return mat_q,new_state

def joue(strategie, space, position_initiale) :
  position_joueur = position_initiale
  end = False
  win = False
  nbCoup = 0
  while end==False and nbCoup<100 :
    action = strategie[position_joueur]

    position_joueur, reward, end = application_action(action, position_joueur, space)
    nbCoup += 1
    if(end) :
      win = np.isin(jewels, [position_joueur])[0]
  #print("Win = ", win)
  return win



def initialisation() :
  plateau = np.reshape(space(taille), (largeur, longueur), order="C")
  mat=random_mat_q()
  return plateau,mat



def training(mat):
  for i in range(NPartie) :
    state = position_initial
    nbCoup=0
    end = False
    win = False
    while end==False and nbCoup<100 :
      epsilon = NPartie/(NPartie+i)
      mat,state=onestep(mat,state,epsilon)
      nbCoup += 1
      
      end = np.isin(fin, [state])[0]
      if(end) :
        win = np.isin(jewels, [state])[0]
    #print("Partie = ", i, " ; nbCoupsTotal = ", nbCoup, " ; win = ", win)
    mat_opti = np.zeros(taille)
    for i in range(taille) :
      mat_opti[i] = np.argmax(mat[i])

    return mat_opti


def doGraphNumberTraining():
  x = [1,10,20,50,100,200]
  y=[]
  for x_value in x:
    nb_reussite = 0
    for i_count in range (nbEchantillons):
      mat = random_mat_q()
      for i in range(x_value) :
        state = position_initial
        nbCoup=0
        end = False
        win = False
        while end==False and nbCoup<100 :
          epsilon = x_value/(x_value+i)
          mat,state=onestep(mat,state,epsilon)
          nbCoup += 1
          
          end = np.isin(fin, [state])[0]
          if(end) :
            win = np.isin(jewels, [state])[0]

      mat_opti = np.zeros(taille)
      for i in range(taille) :
        mat_opti[i] = np.argmax(mat[i])
      if(joue(mat_opti, plateau, position_initial)) :
        nb_reussite=nb_reussite+1

    y.append(nb_reussite/nbEchantillons)

  plt.plot(x, y)
    
  plt.xlabel('nb entrainement')
  plt.ylabel('taux reussite')
  plt.title('Graphe Q-learning!')
    
  plt.show()


plateau,mat=initialisation()
mat_opti = training(mat)
joue(mat_opti, plateau, position_initial)
doGraphNumberTraining()